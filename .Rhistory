# Map continuous affinity to 1..4 ratings with noise
eps <- matrix(rnorm(n_users * n_items, sd = 0.6), n_users, n_items)
score <- scale(affinity + eps)            # standardize
score
affinity
# breakpoints for 1..4
q <- quantile(score, probs = c(.25, .5, .75))
q
to_rating <- function(x, q) {
cut(x, breaks = c(-Inf, q[1], q[2], q[3], Inf), labels = 1:4)
}
ratings_full <- matrix(as.integer(to_rating(score, q)), n_users, n_items)
ratings_full
# Add missingness at random
mask <- matrix(runif(n_users * n_items) < missing_rate, n_users, n_items)
mask
ratings <- ratings_full
ratings[mask] <- NA_integer_
dim(ratings); mean(is.na(ratings))
# Convert to a recommenderlab object:
R <- as(ratings, "realRatingMatrix")
R
ratings
# 0.4 Train/test split & evaluation scheme
idx_test  <- sample(1:n_users, size = 150)
idx_test
R_test    <- R[idx_test, ]
R_train   <- R[-idx_test, ]
# evaluationScheme: 80/20 split on known ratings, 5-fold CV
es <- evaluationScheme(R_train, method = "cross-validation", k = 5, given = -1, goodRating = 4)
es
algos <- list(
"POPULAR" = list(name = "POPULAR", param = NULL), # baseline
"SVD_k20" = list(name = "SVD",     param = list(k = 20, maxiter = 200, normalize = "center")), # k is the latent dimension/factor
"SVD_k40" = list(name = "SVD",     param = list(k = 40, maxiter = 200, normalize = "center"))
)
results <- evaluate(es, method = algos, type = "ratings")
# Summarize RMSE/MAE across folds
perf <- lapply(results, function(res) {
data.frame(
RMSE = avg(res, "RMSE"),
MAE  = avg(res, "MAE")
)
})
do.call(rbind, perf)
# 0.6 Train final model and generate predictions
# Train on all training users, then predict ratings for the held-out test users.
# from the evluate, we pick k=30 and train on all training data
rec <- Recommender(R_train, method = "SVD", parameter = list(k = 30, maxiter = 200, normalize = "center"))
pred_test <- predict(object = rec, newdata = R_test, type = "ratingMatrix")
pred_mat  <- as(pred_test, "matrix")   # numeric predictions
true_mat  <- as(R_test, "matrix")      # ground truth with NAs
pred_test
c(RMSE = rmse, MAE = mae)
pred_mat[1:6, 1:8]
true_mat
pred_mat
co_obs
# Simulate item domains
domains <- factor(sample(c("Cardio","Neuro","ID","Endo","Pulm"), n_items, replace = TRUE))
select_tailored <- function(pred_row, seen_idx = integer(0), n_select = 20,
domains, target_mix = c(Cardio=0.2, Neuro=0.2, ID=0.2, Endo=0.2, Pulm=0.2)) {
# Remove already-seen items
ok <- setdiff(which(!is.na(pred_row)), seen_idx)
cand <- data.frame(item = ok, pred = pred_row[ok], domain = domains[ok], stringsAsFactors = FALSE)
cand <- cand[order(-cand$pred), ]
# Greedy fill by domain proportions
target_counts <- round(target_mix * n_select)
out <- integer(0)
for (d in names(target_counts)) {
need <- target_counts[d]
pool <- cand[cand$domain == d & !(cand$item %in% out), ]
take <- head(pool$item, need)
out <- c(out, take)
}
# If not enough in a domain, top-up from remaining highest predictions
if (length(out) < n_select) {
extra <- setdiff(cand$item, out)
out <- c(out, head(extra, n_select - length(out)))
}
out
}
domains
pred_mat[1, ]
# Example: pick 20 items for test user 1
sel_items <- select_tailored(pred_mat[1, ], seen_idx = which(!is.na(true_mat[1, ])), n_select = 20, domains = domains)
length(sel_items); head(sel_items)
sel_items
df_plot
ggplot(df_plot, aes(x = pred, fill = true)) +
geom_density(alpha = 0.35) +
labs(x = "Predicted relevance", y = "Density", fill = "Observed rating",
title = "Predicted vs. observed relevance (density by true rating)") +
theme_minimal()
# --------------------------------------------------------------------
# ITEM COLD-START (HYBRID): learn V from item content features
# --------------------------------------------------------------------
# 1) Build an item feature matrix X_items (one row per item in V).
#    Example features: blueprint domain (one-hot), difficulty bin, format, etc.
#    Replace this toy example with your real metadata frame 'item_meta' (n_items rows).
#    'item_meta' must have row order aligned with columns of R_train / rows of V.
set.seed(1)
n_items <- nrow(V)
item_meta <- data.frame(
domain = factor(sample(c("Cardio","Neuro","ID","Endo","Pulm"), n_items, TRUE)),
diff_z = rnorm(n_items)  # e.g., standardized difficulty from IRT
)
X_items <- model.matrix(~ domain + diff_z, data = item_meta)  # (n_items x p)
X_items
# 2) Fit p -> V_f for each latent factor f (you can use ridge if high-dim).
#    Here we use simple OLS per factor for clarity; swap to ridge if needed.
coef_list <- vector("list", k)
coef_list
# 2) Fit p -> V_f for each latent factor f (you can use ridge if high-dim).
#    Here we use simple OLS per factor for clarity; swap to ridge if needed.
coef_list <- vector("list", k)
# 2) Fit p -> V_f for each latent factor f (you can use ridge if high-dim).
#    Here we use simple OLS per factor for clarity; swap to ridge if needed.
k=30
coef_list <- vector("list", k)
coef_list
X_items
# 2) Fit p -> V_f for each latent factor f (you can use ridge if high-dim).
#    Here we use simple OLS per factor for clarity; swap to ridge if needed.
k=30
coef_list <- vector("list", k)
for(f in seq_len(k)){
df_f <- data.frame(Vf = V[, f], X_items)
fit  <- lm(Vf ~ . - 1, data = df_f)      # -1: no intercept (already in one-hot)
coef_list[[f]] <- coef(fit)
}
# 2) Fit p -> V_f for each latent factor f (you can use ridge if high-dim).
#    Here we use simple OLS per factor for clarity; swap to ridge if needed.
k=20
coef_list <- vector("list", k)
for(f in seq_len(k)){
df_f <- data.frame(Vf = V[, f], X_items)
fit  <- lm(Vf ~ . - 1, data = df_f)      # -1: no intercept (already in one-hot)
coef_list[[f]] <- coef(fit)
}
# 2) Fit p -> V_f for each latent factor f (you can use ridge if high-dim).
#    Here we use simple OLS per factor for clarity; swap to ridge if needed.
k <- ncol(V)
# 2) Fit p -> V_f for each latent factor f (you can use ridge if high-dim).
#    Here we use simple OLS per factor for clarity; swap to ridge if needed.
k <- ncol(V)  # number of latent factors
coef_list <- vector("list", k)
for(f in seq_len(k)){
df_f <- data.frame(Vf = V[, f], X_items)
fit  <- lm(Vf ~ . - 1, data = df_f)      # -1: no intercept (already in one-hot)
coef_list[[f]] <- coef(fit)
}
# 3) Given a NEW ITEM with content features, predict its V_new (k-vector).
# Example new item metadata:
new_item <- data.frame(domain = factor("Pulm", levels(item_meta$domain)),
diff_z = 0.3)
x_new <- model.matrix(~ domain + diff_z, data = new_item)    # (1 x p)
V_new <- numeric(k)
for(f in seq_len(k)){
# align coefficients to columns in x_new by names
b <- coef_list[[f]]
V_new[f] <- sum(x_new[1, names(b)] * b)
}
item_meta
X_items
V
coef_list
k
# 3) Given a NEW ITEM with content features, predict its V_new (k-vector).
# Example new item metadata:
new_item <- data.frame(domain = factor("Pulm", levels(item_meta$domain)), diff_z = 0.3)
x_new <- model.matrix(~ domain + diff_z, data = new_item)    # (1 x p)
new_item
x_new
V_new
for(f in seq_len(k)){
# align coefficients to columns in x_new by names
b <- coef_list[[f]]
V_new[f] <- sum(x_new[1, names(b)] * b)
}
k
for(f in seq_len(k)){
# align coefficients to columns in x_new by names
b <- coef_list[[f]]
V_new[f] <- sum(x_new[1, names(b)] * b)
}
names(b)
f=8
# align coefficients to columns in x_new by names
b <- coef_list[[f]]
b
V_new[f]
sum(x_new[1, names(b)] * b)
x_new[1, names(b)]
x_new
x_new
V_new <- nrow(x_new)
V_new
for(f in seq_len(k)){
# align coefficients to columns in x_new by names
b <- coef_list[[f]]
V_new[f] <- sum(x_new[1, names(b)] * b)
}
V_new <- k <- nrow(x_new)
for(f in seq_len(k)){
# align coefficients to columns in x_new by names
b <- coef_list[[f]]
V_new[f] <- sum(x_new[1, names(b)] * b)
}
f=2
# align coefficients to columns in x_new by names
b <- coef_list[[f]]
V_new[f] <- sum(x_new[1, names(b)] * b)
X_items
# --- 3) New item -> build x_new with the SAME formula and levels
new_item <- data.frame(
domain = factor("Pulm", levels = levels(item_meta$domain)),
diff_z = 0.3
)
x_new_raw <- model.matrix(~ domain + diff_z - 1, data = new_item)
# 3) Given a NEW ITEM with content features, predict its V_new (k-vector).
# New item -> build x_new with the SAME formula and levels
new_item <- data.frame(
domain = factor("Pulm", levels = levels(item_meta$domain)),
diff_z = 0.3
)
x_new_raw <- model.matrix(~ domain + diff_z - 1, data = new_item)
# Force identical column set as training (fill missing with 0)
x_new <- matrix(0, nrow = 1, ncol = length(train_cols),
dimnames = list(NULL, train_cols))
x_new[, colnames(x_new_raw)] <- x_new_raw
new_item
x_new_raw
x_new
x_new
# --- 4) Align each coef vector to training columns and predict V_new
V_new <- numeric(k)
for (f in seq_len(k)) {
b <- coef_list[[f]]                      # named vector (subset of train_cols)
# expand to full column set with zeros for missing
b_full <- setNames(numeric(length(train_cols)), train_cols)
b_full[names(b)] <- b
V_new[f] <- drop(x_new %*% b_full)       # safe dot product
}
train_cols
# Force identical column set as training (fill missing with 0)
x_new <- matrix(0, nrow = 1, ncol = length(train_cols),
dimnames = list(NULL, train_cols))
R_train
# Ensure x_new has the same columns as X_items (training)
train_cols <- colnames(X_items)  # columns used in training
# Force identical column set as training (fill missing with 0)
x_new <- matrix(0, nrow = 1, ncol = length(train_cols),
dimnames = list(NULL, train_cols))
x_new[, colnames(x_new_raw)] <- x_new_raw
x_new
x_new[, colnames(x_new_raw)]
x_new_raw
colnames(x_new_raw)
x_new
x_new
x_new[1:nrow(x_new_raw), ] <- x_new_raw
x_new
x_new
# --- 4) Align each coef vector to training columns and predict V_new
V_new <- numeric(k)
for (f in seq_len(k)) {
b <- coef_list[[f]]                      # named vector (subset of train_cols)
# expand to full column set with zeros for missing
b_full <- setNames(numeric(length(train_cols)), train_cols)
b_full[names(b)] <- b
V_new[f] <- drop(x_new %*% b_full)       # safe dot product
}
V_new
k
b <- coef_list[[f]]                      # named vector (subset of train_cols)
# expand to full column set with zeros for missing
b_full <- setNames(numeric(length(train_cols)), train_cols)
b_full[names(b)] <- b
V_new[f] <- drop(x_new %*% b_full)       # safe dot product
b_full
setNames(numeric(length(train_cols)), train_cols)
b
b_full[names(b)] <- b
b_full
b
x_new
drop(x_new %*% b)
x_new
b
x_new %*% b
# 4) Score this new item for ALL existing users:
# Interaction term on the centered scale:
aff_all_users <- as.numeric(U %*% V_new)   # length = n_users
V_new
for (f in seq_len(k)) {
b <- coef_list[[f]]                      # named vector (subset of train_cols)
V_new[f] <- drop(x_new %*% b)       # dot product and convert to scalar
}
V_new
as.numeric(U %*% V_new)
View(U)
b
# Example: predicted rating for user i0 on the NEW item:
i0 <- 1
pred_newitem_user_i0 <- pred_rating_all_users[i0]
aff_all_users <- as.numeric(U %*% V_new)   # length = n_users
# 2) Fit p -> V_f for each latent factor f (you can use ridge if high-dim).
#    Here we use simple OLS per factor for clarity; swap to ridge if needed.
k <- ncol(V)  # number of latent factors
k
# --- 4) Align each coef vector to training columns and predict V_new
V_new <- numeric(k)
V_new
for (f in seq_len(k)) {
b <- coef_list[[f]]                      # named vector (subset of train_cols)
V_new[f] <- drop(x_new %*% b)       # dot product and convert to scalar
}
# 4) Score this new item for ALL existing users:
# Interaction term on the centered scale:
aff_all_users <- as.numeric(U %*% V_new)   # length = n_users
# Map back to original scale (add user means from training centering):
pred_rating_all_users <- aff_all_users + user_means
user_means
aff_all_users
# User mean ratings (to re-add after "center" normalization)
user_means <- rowMeans(as(R_train, "matrix"), na.rm = TRUE)
user_means
# Map back to original scale (add user means from training centering):
pred_rating_all_users <- aff_all_users + user_means
# Example: predicted rating for user i0 on the NEW item:
i0 <- 1
pred_newitem_user_i0 <- pred_rating_all_users[i0]
pred_newitem_user_i0
# Map back to original scale (add user means from training centering):
pred_rating_all_users <- aff_all_users + user_means
rasch <- 1/(1 + exp(-(t - b)))
I_rsasch <- function(t, b) {
p <- rasch(t, b)
return(p * (1 - p))
}
b_a = seq(-3, 3, length.out = 200)
b_b = seq(-3, 1.5, length.out = 200)
I_a = I_rsasch(t = 0, b = b_a)
I_b = I_rsasch(t = 1, b = b_b)
test_I_a = sum(I_a)
b_a
b_b
I_a = I_rsasch(t = 0, b = b_a)
rasch <- 1/(1 + exp(-(t - b)))
rasch <- functionm(t, b){1/(1 + exp(-(t - b)))}
I_rsasch <- function(t, b) {
p <- rasch(t, b)
return(p * (1 - p))
}
b_a = seq(-3, 3, length.out = 200)
b_b = seq(-3, 1.5, length.out = 200)
I_a = I_rsasch(t = 0, b = b_a)
I_b = I_rsasch(t = 1, b = b_b)
test_I_a = sum(I_a)
test_I_b = sum(I_b)
b_a
I_a
I_rsasch(t = 0, b = b_a)
rasch <- function(t, b){1/(1 + exp(-(t - b)))}
I_rsasch <- function(t, b) {
p <- rasch(t, b)
return(p * (1 - p))
}
b_a = seq(-3, 3, length.out = 200)
b_b = seq(-3, 1.5, length.out = 200)
I_a = I_rsasch(t = 0, b = b_a)
I_b = I_rsasch(t = 1, b = b_b)
test_I_a = sum(I_a)
test_I_b = sum(I_b)
test_I_a
test_I_b
sem_a = 1/sqrt(test_I_a)
sem_b = 1/sqrt(test_I_b)
sem_a
sem_b
rel_a = 1 - sem_a^2
rel_b = 1 - sem_b^2
rel_a
rel_b
b_a = seq(-1, 1, length.out = 200)
b_b = seq(-3, 1.5, length.out = 200)
I_a = I_rsasch(t = 0, b = b_a)
I_b = I_rsasch(t = 1, b = b_b)
test_I_a = sum(I_a)
test_I_b = sum(I_b)
sem_a = 1/sqrt(test_I_a)
sem_b = 1/sqrt(test_I_b)
rel_a = 1 - sem_a^2
rel_b = 1 - sem_b^2
rel_a
rel_b
2.76/0.14
0.39/0.14
sqrt(0.1568)
2.76*2.76/(1+2.76*2.76)
(0.42*0.42-0.14*0.14)/(0.42*0.42)
# item_info(theta) should return sum of item informations at theta
# define theta grid and population density
theta <- seq(-4, 4, by = 0.01)
f_theta <- dnorm(theta, mean = 0, sd = 1)  # population
# example Rasch info if you don't have items handy
rasch_item_info <- function(b, th) { p <- plogis(th - b); p*(1-p) }  # 1PL: a=1
item_bs <- c(-2,-1,0,1,2)  # example
item_info <- function(th) sum(rasch_item_info(item_bs, th))
SEM <- 1 / sqrt(sapply(theta, item_info))
theta_c <- 0  # pass if hat{theta} >= 0
p_pass <- 1 - pnorm((theta_c - theta)/SEM)
cc_theta <- p_pass^2 + (1 - p_pass)^2
CC <- sum(cc_theta * f_theta) / sum(f_theta)
round(CC, 3)
item_info
SEM
theta
theta_c <- 0  # pass if hat{theta} >= 0
p_pass <- 1 - pnorm((theta_c - theta)/SEM)
p_pass
cc_theta
sum(f_theta)
sum(cc_theta * f_theta)
round(CC, 3)
sum(f_theta)
f_theta
cc_theta
plot(theta, cc_theta, type = 'l', xlab = expression(theta), ylab = "CC")
pnorm((theta_c - theta)/SEM)
p_pass
pnorm((theta_c - theta)/SEM)
plot(theta, p_pass, type = 'l', xlab = expression(theta), ylab = "P(pass)")
plot(theta, SEM, type = 'l', xlab = expression(theta), ylab = "SEM")
plot(theta, pnorm((theta_c - theta)/SEM), type = 'l', xlab = expression(theta), ylab = "P(fail)")
qnorm(0)
qnorm(0.25)
qnorm(0.9)
qnorm(0.99)
qnorm(0.999)
qnorm(0.9999)
qnorm(0.999999999999999)
pnorm(0)
pnorm(0.8)
pnorm(1.8)
dnorm(0)
dnorm(0.5)
dnorm(0.9)
cc_theta
sqrt(0.33*0.33-0.15*0.15)
sqrt(0.32*0.32-0.15*0.15)
setwd("C:/Users/JakeCho/Projects/Conferences/ICE Exchange2025Nov/ ")
## Copyright (c) Meazure Learning, 2025
##
## Email: jcho@meazurelearning.com
##
## ---------------------------
##
## Notes:
##   - <add notes or dependencies>
##
## ---------------------------
library(openxlsx)
library(tidyverse)
setwd("C:/Users/JakeCho/Projects/Conferences/ICE Exchange2025Nov/")
f1 <- read.xlsx("AI_ATA_forms_assembly", sheet = "Form_1")
f2 <- read.xlsx("AI_ATA_forms_assembly", sheet = "Form_2")
f3 <- read.xlsx("AI_ATA_forms_assembly", sheet = "Form_3")
setwd("C:/Users/JakeCho/Projects/Conferences/ICE Exchange2025Nov/")
f1 <- read.xlsx("AI_ATA_forms_assembly", sheet = "Form_1")
f1 <- read.xlsx("AI_ATA_forms_assembly.xlsx", sheet = "Form_1")
f2 <- read.xlsx("AI_ATA_forms_assembly.xlsx", sheet = "Form_2")
f3 <- read.xlsx("AI_ATA_forms_assembly.xlsx", sheet = "Form_3")
#count the number of unique domians in each form
table(f1$Domain)
table(f2$Domain)
table(f3$Domain)
#count the number of unique domians in each form
table(f1$domain)
table(f2$domain)
table(f3$domain)
mean(f1$rasch_b)
mean(f2$rasch_b)
mean(f3$rasch_b)
mean(f1$point_biserial)
mean(f2$point_biserial)
mean(f3$point_biserial)
#find the common items across the three forms and between two forms
common_items_1_2 <- intersect(f1$item_id, f2$item_id)
common_items_1_3 <- intersect(f1$item_id, f3$item_id)
common_items_2_3 <- intersect(f2$item_id, f3$item_id)
common_items_all <- Reduce(intersect, list(f1$item_id, f2$item_id, f3$item_id))
common_items_1_2
common_items_1_3
common_items_2_3 <- intersect(f2$item_id, f3$item_id)
common_items_2_3
common_items_all
#check the all four common items are identical
f1_common <- f1 %>% filter(item_id %in% common_items_all) %>% arrange(item_id)
f2_common <- f2 %>% filter(item_id %in% common_items_all) %>% arrange(item_id)
f3_common <- f3 %>% filter(item_id %in% common_items_all) %>% arrange(item_id)
all.equal(f1_common, f2_common)
all.equal(f1_common, f3_common)
all.equal(f2_common, f3_common)
# coutn the unique items in each form
unique_items_f1 <- setdiff(f1$item_id, union(f2$item_id, f3$item_id))
unique_items_f2 <- setdiff(f2$item_id, union(f1$item_id, f3$item_id))
unique_items_f3 <- setdiff(f3$item_id, union(f1$item_id, f2$item_id))
unique_items_f1
unique_items_f2 <- setdiff(f2$item_id, union(f1$item_id, f3$item_id))
unique_items_f3 <- setdiff(f3$item_id, union(f1$item_id, f2$item_id))
